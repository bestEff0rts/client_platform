data.columns.name = None  # убрать name='Price'
data.index.name = "Date"  # явно
data = data.droplevel(0, axis=1)  # если это мультииндекс по колонкам, убери "Price"
data = data.astype(float)
data
data.columns.name = None
data
data = data.droplevel(0, axis=1)  # если это мультииндекс по колонкам, убери "Price"
data = data.droplevel(Price, axis=1)  # если это мультииндекс по колонкам, убери "Price"
data
data.columns.name = ["close", "high", "low", "open", "volume"]
import yfinance as yf
data = yf.download('AAPL', start='2007-12-01', end='2008-12-01')
data.columns.name = None  # убрать name='Price'
data.index.name = "Date"  # явно
data = data.droplevel(0, axis=1)  # если это мультииндекс по колонкам, убери "Price"
data = data.astype(float)
data = data.sort_index()
data
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from UniStatTransf import UniStationarityTransformer  # кастомные трансформеры
def build_preprocessing_pipeline():
return Pipeline([
("features", FeatureEngineeringTransformer()),
("stationarity", UniStationarityTransformer(columns=[
"open", "high", "low", "close", "volume", "log_returns",
"volatility_20", "vwap", "sma_5", "sma_20", "rsi"
])),
("scaler", StandardScaler())
])
from myfucntions import build_preprocessing_pipeline
import os
os.chdir("D:\project_root")
from myfunctions import build_preprocessing_pipeline
from UniStatTransf import UniStationarityTransformer
preprocessing_pipe_x = build_preprocessing_pipeline()
from myparser import fetch_binance_ohlcv, fetch_moex_ohlcv
import asyncio
data = asyncio.run(fetch_binance_ohlcv("AAVE/USDT")) #___/___; не тикер а trading pair
len(data)
data
df = pd.DataFrame(data, columns=["timestamp", "open", "high", "low", "close", "volume"])
df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms") #timestamp → datetime
print(df.describe())
df["target"]= df["close"].diff() ##РАСЧЕТ TARGET = Rt(returns or log returns) ВНЕ пайплайна
print(df.describe())
preprocessing_pipe_x = build_preprocessing_pipeline()
x_transformed = preprocessing_pipe_x.fit_transform(x)
x = df.drop(columns=["target", "timestamp"])
y = df["target"]
x.describe()
preprocessing_pipe_x = build_preprocessing_pipeline()
x_transformed = preprocessing_pipe_x.fit_transform(x)
x_transformed_df = pd.DataFrame(x_transformed, index=x.index[-len(x_transformed):]) #из np array в pd df сохр временную структуру для сплита
x_transformed_df.columns = ["open","high","low", "close","volume","log_returns", "volatility_20", "vwap", "sma_5", "sma_20", "rsi"]
x_transformed_df.describe()
from myModelFactory import build_model
model = build_model("svm") #ФАБРИКА МОДЕЛЕЙ myModelFactory.py
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=20,
scoring='f1_macro',
cv=5,
random_state=69
)
from sklearn.model_selection import RandomizedSearchCV.
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=20,
scoring='f1_macro',
cv=5,
random_state=69
)
assert not x_transformed_df.isnull().values.any(), "NaNs detected in x"
assert not x.isnull().values.any(), "NaNs detected in x"
param_grids = {
'random_forest': {
'n_estimators': [100, 200],
'max_depth': [None, 10],
'min_samples_split': [2, 5]
},
'xgboost': {
'n_estimators': [100, 200],
'max_depth': [3, 6],
'learning_rate': [0.01, 0.1]
},
'logistic_regression': {
'C': [0.1, 1.0, 10],
'penalty': ['l1', 'l2'],
'solver': ['liblinear']
}
'svm': {
'C': [0.1, 1, 10],
'kernel': ['linear', 'rbf'],
'gamma': ['scale', 'auto']
},
'lasso': {
'alpha': [0.01, 0.1, 1.0, 10],
'max_iter': [1000, 5000]
},
'ridge': {
'alpha': [0.01, 0.1, 1.0, 10],
'solver': ['auto', 'svd', 'cholesky']
},
'pls': {
'n_components': [2, 4, 6, 8]
}
}
param_grid = param_grids[xgboost]
param_grids = {
'random_forest': {
'n_estimators': [100, 200],
'max_depth': [None, 10],
'min_samples_split': [2, 5]
},
'xgboost': {
'n_estimators': [100, 200],
'max_depth': [3, 6],
'learning_rate': [0.01, 0.1]
},
'logistic_regression': {
'C': [0.1, 1.0, 10],
'penalty': ['l1', 'l2'],
'solver': ['liblinear']
}
'svm': {
'C': [0.1, 1, 10],
'kernel': ['linear', 'rbf'],
'gamma': ['scale', 'auto']
from myfunctions import build_preprocessing_pipeline
from UniStatTransf import UniStationarityTransformer
from myparser import fetch_binance_ohlcv, fetch_moex_ohlcv
import asyncio
data = asyncio.run(fetch_binance_ohlcv("AAVE/USDT")) #___/___; не тикер а trading pair
len(data)
data
df = pd.DataFrame(data, columns=["timestamp", "open", "high", "low", "close", "volume"])
df["timestamp"] = pd.to_datetime(df["timestamp"], unit="ms") #timestamp → datetime
print(df.describe())
df["target"]= df["close"].diff() ##РАСЧЕТ TARGET = Rt(returns or log returns) ВНЕ пайплайна
x = df.drop(columns=["target", "timestamp"])
y = df["target"]
x.describe()
assert not x.isnull().values.any(), "NaNs detected in x"
preprocessing_pipe_x = build_preprocessing_pipeline()
x_transformed = preprocessing_pipe_x.fit_transform(x)
x_transformed_df = pd.DataFrame(x_transformed, index=x.index[-len(x_transformed):]) #из np array в pd df сохр временную структуру для сплита
x_transformed_df.columns = ["open","high","low", "close","volume","log_returns", "volatility_20", "vwap", "sma_5", "sma_20", "rsi"]
x_transformed_df.describe()
from myModelFactory import build_model
model = build_model("svm") #ФАБРИКА МОДЕЛЕЙ myModelFactory.py
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=20,
scoring='f1_macro',
cv=TimeSeriesSplit(n_splits=5), #TimeSeriesSplit(n_splits=5)-StratifiedKFold
random_state=69
)
param_grid = param_grids[xgboost]
param_grid = param_grids['xgboost']
param_grid = param_grids["xgboost""]
param_grid = param_grids["xgboost"]
param_grids = {
'random_forest': {
'n_estimators': [100, 200],
'max_depth': [None, 10],
'min_samples_split': [2, 5]
},
'xgboost': {
'n_estimators': [100, 200],
'max_depth': [3, 6],
'learning_rate': [0.01, 0.1]
},
'logistic_regression': {
'C': [0.1, 1.0, 10],
'penalty': ['l1', 'l2'],
'solver': ['liblinear']
}
'svm': {
'C': [0.1, 1, 10],
'kernel': ['linear', 'rbf'],
'gamma': ['scale', 'auto']
},
'lasso': {
'alpha': [0.01, 0.1, 1.0, 10],
'max_iter': [1000, 5000]
},
'ridge': {
'alpha': [0.01, 0.1, 1.0, 10],
'solver': ['auto', 'svd', 'cholesky']
},
'pls': {
'n_components': [2, 4, 6, 8]
}
}
param_grid = param_grids["xgboost"]
param_grids = {
'random_forest': {
'n_estimators': [100, 200],
'max_depth': [None, 10],
'min_samples_split': [2, 5]
},
'xgboost': {
'n_estimators': [100, 200],
'max_depth': [3, 6],
'learning_rate': [0.01, 0.1]
},
'logistic_regression': {
'C': [0.1, 1.0, 10],
'penalty': ['l1', 'l2'],
'solver': ['liblinear']
},
'svm': {
'C': [0.1, 1, 10],
'kernel': ['linear', 'rbf'],
'gamma': ['scale', 'auto']
},
'lasso': {
'alpha': [0.01, 0.1, 1.0, 10],
'max_iter': [1000, 5000]
},
'ridge': {
'alpha': [0.01, 0.1, 1.0, 10],
'solver': ['auto', 'svd', 'cholesky']
},
'pls': {
'n_components': [2, 4, 6, 8]
}
}
param_grid = param_grids["xgboost"]
from myModelFactory import build_model
model = build_model("svm") #ФАБРИКА МОДЕЛЕЙ myModelFactory.py
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import RandomizedSearchCV
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=20,
scoring='f1_macro',
cv=TimeSeriesSplit(n_splits=5), #TimeSeriesSplit(n_splits=5)-StratifiedKFold
random_state=69
)
from sklearn.model_selection import TimeSeriesSplit
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=20,
scoring='f1_macro',
cv=TimeSeriesSplit(n_splits=5), #TimeSeriesSplit(n_splits=5)-StratifiedKFold
random_state=69
)
random_search.fit(X_transformed_df, y)
random_search.fit(x_transformed_df, y)
y = y[-len(x_transformed):]  # обрезать y, чтобы соответствовал X
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=20,
scoring='f1_macro',
cv=TimeSeriesSplit(n_splits=5), #TimeSeriesSplit(n_splits=5)-StratifiedKFold
random_state=69
)
random_search.fit(x_transformed_df, y)
param_grid = param_grids["svm"]
from myModelFactory import build_model
model = build_model("svm") #ФАБРИКА МОДЕЛЕЙ myModelFactory.py
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import TimeSeriesSplit
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=20,
scoring='f1_macro',
cv=TimeSeriesSplit(n_splits=5), #TimeSeriesSplit(n_splits=5)-StratifiedKFold
random_state=69
)
random_search.fit(x_transformed_df, y)
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=20,
#scoring='f1_macro',
cv=TimeSeriesSplit(n_splits=5), #TimeSeriesSplit(n_splits=5)-StratifiedKFold
random_state=69
)
random_search.fit(x_transformed_df, y)
random_search = RandomizedSearchCV(
estimator=model,
param_distributions=param_grid,
n_iter=12,
#scoring='f1_macro',
cv=TimeSeriesSplit(n_splits=5), #TimeSeriesSplit(n_splits=5)-StratifiedKFold
random_state=69
)
random_search.fit(x_transformed_df, y)
random_search.fit(x_transformed_df, y).get_params()
best_model = random_search.best_estimator_
print(best_model.get_params())
train_size = int(len(x_transformed_df) * 0.8)
X_train, X_test = x_transformed_df[:train_size], x_transformed_df[train_size:]
y_train, y_test = y[:train_size], y[train_size:]
from sklearn.metrics import r2_score, mean_squared_error, roc_auc_score, roc_curve
y_pred = best_model.predict(X_test)
if hasattr(best_model, "predict_proba"):
y_proba = best_model.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_proba)
print(f"AUC: {auc:.4f}")
else:
# Для регрессии
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
print(f"R²: {r2:.4f}")
print(f"MSE: {mse:.4f}")
def getDailyVol(close,span0=100):
# daily vol, reindexed to close
df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))
def getDailyVol(close,span0=100):
# daily vol, reindexed to close
df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))
df0=df0[df0>0]
df0=pd.Series(close.index[df0–1], index=close.index[close.shape[0]-df0.shape[0]:])
df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns
df0=df0.ewm(span=span0).std()
return df0
def getDailyVol(close,span0=100):# daily vol, reindexed to close
df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))
df0=df0[df0>0]
df0=pd.Series(close.index[df0–1], index=close.index[close.shape[0]-df0.shape[0]:])
df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns
df0=df0.ewm(span=span0).std()
return df0
def getDailyVol(close,span0=100):# daily vol, reindexed to close
df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))
df0=df0[df0>0]
df0=pd.Series(close.index[df0–1], index=close.index[close.shape[0]-df0.shape[0]:])
df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns
df0=df0.ewm(span=span0).std()
return df0
def getDailyVol(close,span0=100):
df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))
df0=df0[df0>0]
df0=pd.Series(close.index[df0–1], index=close.index[close.shape[0]-df0.shape[0]:])
df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns
df0=df0.ewm(span=span0).std()
return df0
def getDailyVol(close,span0=100):
df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))
df0=df0[df0>0]
df0=pd.Series(close.index[df0–1], index=close.index[close.shape[0]-df0.shape[0]:])
df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns
df0=df0.ewm(span=span0).std()
return df0
def getDailyVol(close,span0=100):
df0=close.index.searchsorted(close.index-pd.Timedelta(days=1))
df0=df0[df0>0]
df0=pd.Series(close.index[df0–1], index=close.index[close.shape[0]-df0.shape[0]:])
df0=close.loc[df0.index]/close.loc[df0.values].values-1 # daily returns
df0=df0.ewm(span=span0).std()
df["target_class"] = (df["close"].shift(-1) > df["close"]).astype(int)
import pandas as pd
def get_daily_vol(close: pd.Series, span0: int = 100) -> pd.Series:
"""
Calculate daily volatility (standard deviation of daily returns, exponentially weighted).
Parameters:
-----------
close : pd.Series
Series of closing prices indexed by datetime.
span0 : int
Span for the exponentially weighted moving std.
Returns:
--------
pd.Series
Daily volatility estimate.
"""
# Найти индексы предыдущего дня
df0 = close.index.searchsorted(close.index - pd.Timedelta(days=1))
df0 = df0[df0 > 0]  # исключить нулевые смещения
# Создать серию: индекс — текущая дата, значения — дата T-1
prev_day_idx = pd.Series(close.index[df0 - 1], index=close.index[-df0.shape[0]:])
# Вычислить доходности: close[t] / close[t-1] - 1
daily_ret = close.loc[prev_day_idx.index] / close.loc[prev_day_idx.values].values - 1
# EWMA стандартное отклонение доходностей
daily_vol = daily_ret.ewm(span=span0).std()
return daily_vol
def apply_pt_sl_on_t1(close: pd.Series,
events: pd.DataFrame,
pt_sl: tuple,
molecule: list) -> pd.DataFrame:
"""
Применение profit-taking и stop-loss барьеров до момента t1 (конца события).
Parameters:
-----------
close : pd.Series
Временной ряд цен.
events : pd.DataFrame
DataFrame с колонками ['t1', 'trgt', 'side'].
pt_sl : tuple
Кортеж вида (pt, sl), где pt/sl — множители таргета.
molecule : list
Подмножество индексов событий (молекула) для асинхронной обработки.
Returns:
--------
pd.DataFrame
DataFrame с колонками 'pt' и 'sl' — временем срабатывания барьеров.
"""
# Скопировать подмножество событий
events_ = events.loc[molecule]
out = events_[['t1']].copy(deep=True)
# Задать уровни PT/SL (в абсолютных значениях, умноженные на таргет)
pt = pt_sl[0] * events_['trgt'] if pt_sl[0] > 0 else pd.Series(index=events_.index)
sl = -pt_sl[1] * events_['trgt'] if pt_sl[1] > 0 else pd.Series(index=events_.index)
# Обход событий: найти первое достижение PT/SL или конец события t1
for loc, t1 in events_['t1'].fillna(close.index[-1]).items():
# Путь цены от начала до конца события
price_path = close[loc:t1]
# Возвраты от точки входа, с учётом направления (side)
price_returns = (price_path / close[loc] - 1) * events_.at[loc, 'side']
# Находим первую дату, когда был достигнут SL / PT
out.loc[loc, 'sl'] = price_returns[price_returns < sl[loc]].index.min()
out.loc[loc, 'pt'] = price_returns[price_returns > pt[loc]].index.min()
return out
from typing import Union
def get_events(close: pd.Series,
t_events: pd.Index,
pt_sl: float,
trgt: pd.Series,
min_ret: float,
num_threads: int,
t1: Union[bool, pd.Series] = False) -> pd.DataFrame:
"""
Выделяет события с учетом порогов доходности и горизонта удержания.
Parameters:
-----------
close : pd.Series
Цены закрытия (индекс — datetime).
t_events : pd.Index
Индексы (даты) предполагаемых событий (например, bars с фильтрацией).
pt_sl : float
Множитель таргета для установки PT и SL (одинаковый).
trgt : pd.Series
Целевая переменная (target) — ожидаемое движение цены.
min_ret : float
Минимальное значение target, ниже которого событие игнорируется.
num_threads : int
Число потоков для multiprocessing (используется в mpPandasObj).
t1 : Union[bool, pd.Series]
Горизонт максимального удержания события. Если False — будет заполняться NaT.
Returns:
--------
events : pd.Dat
def get_events(close: pd.Series,
t_events: pd.Index,
pt_sl: float,
trgt: pd.Series,
min_ret: float,
num_threads: int,
t1: Union[bool, pd.Series] = False) -> pd.DataFrame:
"""
Выделяет события с учетом порогов доходности и горизонта удержания.
Parameters:
-----------
close : pd.Series
Цены закрытия (индекс — datetime).
t_events : pd.Index
Индексы (даты) предполагаемых событий (например, bars с фильтрацией).
pt_sl : float
Множитель таргета для установки PT и SL (одинаковый).
trgt : pd.Series
Целевая переменная (target) — ожидаемое движение цены.
min_ret : float
Минимальное значение target, ниже которого событие игнорируется.
num_threads : int
Число потоков для multiprocessing (используется в mpPandasObj).
t1 : Union[bool, pd.Series]
Горизонт максимального удержания события. Если False — будет заполняться NaT.
Returns:
--------
events : pd.DataFrame
DataFrame с колонками: 't1', 'trgt'
"""
# 1) Отфильтровать по min_ret
trgt = trgt.loc[t_events]
trgt = trgt[trgt > min_ret]
# 2) Установить t1 если не передан
if t1 is False:
t1 = pd.Series(pd.NaT, index=trgt.index)
# 3) Создать события
side_ = pd.Series(1.0, index=trgt.index)  # лонг по умолчанию
events = pd.concat({'t1': t1, 'trgt': trgt, 'side': side_}, axis=1).dropna(subset=['trgt'])
# 4) Применить PT/SL барьеры (через multiprocessing)
from mlfinlab.util.multiprocess import mp_pandas_obj  # или твоя обёртка
df0 = mp_pandas_obj(func=apply_pt_sl_on_t1,
pd_obj=('molecule', events.index),
num_threads=num_threads,
close=close,
events=events,
pt_sl=[pt_sl, pt_sl])
# 5) t1 = минимум между PT и SL (первая сработка)
events['t1'] = df0.dropna(how='all').min(axis=1)
# 6) Удалить колонку side
events = events.drop('side', axis=1)
return events
import pandas as pd
import pandas as pd
reticulate::repl_python()
reticulate::repl_python()
